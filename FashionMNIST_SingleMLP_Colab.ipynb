{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Fashion-MNIST \u2022 Single MLP (Sequential) \u2014 Colab Notebook\n", "\n", "## Problem Statement\n", "Build and interpret a **single Multi-Layer Perceptron (MLP)** classifier for **Fashion-MNIST**.  \n", "You will understand **why we flatten images (28\u00d728 \u2192 784)** for MLPs, how **scaling** affects learning, and how to interpret **metrics and errors**.\n", "\n", "## Learning Objectives\n", "- Load Fashion-MNIST and **inspect structure** with Pandas (`head()` & `info()`).\n", "- **Visualize** sample images, class distribution, and pixel histograms (**before/after scaling**).\n", "- Understand **Flatten vs Image Grid** and **what an MLP is**.\n", "- Build, train, and evaluate a **single MLP (Sequential)**.\n", "- Plot **training curves**, **confusion matrix**, and review **misclassifications**."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 1) Setup & Imports"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["import numpy as np\n", "import pandas as pd\n", "import matplotlib.pyplot as plt\n", "\n", "import tensorflow as tf\n", "from tensorflow import keras\n", "from tensorflow.keras import layers\n", "\n", "from sklearn.metrics import confusion_matrix, classification_report\n", "\n", "print(\"TensorFlow:\", tf.__version__)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 2) Load Dataset (Fashion-MNIST)\n", "- Images: 28\u00d728 **grayscale** (shape `(N, 28, 28)`), pixel values **0..255** (uint8).  \n", "- Labels: integers **0..9** mapping to clothing categories."]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["from tensorflow.keras.datasets import fashion_mnist\n", "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n", "\n", "class_names = [\n", "    \"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n", "    \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"\n", "]\n", "\n", "print(\"\\nShapes & dtypes:\")\n", "print(\"  train_images:\", train_images.shape, train_images.dtype)   # (60000, 28, 28) uint8\n", "print(\"  train_labels:\", train_labels.shape, train_labels.dtype)   # (60000,) uint8\n", "print(\"  test_images :\", test_images.shape,  test_images.dtype)    # (10000, 28, 28)\n", "print(\"  test_labels :\", test_labels.shape,  test_labels.dtype)\n", "print(\"Pixel range BEFORE scaling:\", int(train_images.min()), \"to\", int(train_images.max()))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 3) Inspect with Pandas (Head & Info)\n", "Flatten a **sample** to a table (28\u00d728 \u2192 784 columns) to make the data feel like a CSV."]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["N_SAMPLE = 2000\n", "X_flat_sample = train_images[:N_SAMPLE].reshape(N_SAMPLE, -1)\n", "df = pd.DataFrame(X_flat_sample)\n", "df[\"label\"] = train_labels[:N_SAMPLE]\n", "\n", "print(\"\\n--- Pandas HEAD (first 5 rows) ---\")\n", "print(df.head())\n", "\n", "print(\"\\n--- Pandas INFO ---\")\n", "df.info()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 4) Visualize Samples & Class Distribution"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["def show_grid(images, labels, rows=3, cols=6, title=\"Sample training images (raw)\"):\n", "    plt.figure(figsize=(cols*2.0, rows*2.0))\n", "    for i in range(rows*cols):\n", "        plt.subplot(rows, cols, i+1)\n", "        plt.imshow(images[i], cmap=\"gray\")\n", "        plt.title(class_names[int(labels[i])], fontsize=9)\n", "        plt.axis(\"off\")\n", "    plt.suptitle(title)\n", "    plt.tight_layout()\n", "    plt.show()\n", "\n", "show_grid(train_images, train_labels, rows=3, cols=6)\n", "\n", "vals, cnts = np.unique(train_labels, return_counts=True)\n", "plt.figure(figsize=(7,4))\n", "plt.bar(vals, cnts)\n", "plt.xlabel(\"Class index\")\n", "plt.ylabel(\"Count\")\n", "plt.title(\"Label distribution (training set)\")\n", "plt.xticks(vals, class_names, rotation=45, ha=\"right\")\n", "plt.tight_layout()\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 5) Why Scaling? (0..255 \u2192 0..1)\n", "Neural nets train faster and more stably when features share similar ranges.  \n", "We visualize pixel histograms **before** scaling to see the raw spread."]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["rand_pixels = train_images[:500].reshape(-1)  # 500 images worth of pixels\n", "plt.figure(figsize=(6,4))\n", "plt.hist(rand_pixels, bins=30)\n", "plt.xlabel(\"Pixel intensity (0..255)\")\n", "plt.ylabel(\"Frequency\")\n", "plt.title(\"Pixel histogram BEFORE scaling\")\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 6) Preprocessing: Scale, Flatten, Validation Split\n", "- **Scale** to `[0,1]` by dividing by 255.  \n", "- **Flatten** each image to **784 features** (needed by MLP).  \n", "- Make a **validation split** from the training set."]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# Scale to [0,1]\n", "train_images = train_images.astype(\"float32\") / 255.0\n", "test_images  = test_images.astype(\"float32\")  / 255.0\n", "\n", "# Flatten to vectors (28*28 = 784)\n", "x_train = train_images.reshape(len(train_images), -1)\n", "x_test  = test_images.reshape(len(test_images),   -1)\n", "\n", "# Validation split (10%)\n", "VAL_FRAC = 0.1\n", "val_size = int(len(x_train) * VAL_FRAC)\n", "x_val, y_val = x_train[:val_size], train_labels[:val_size]\n", "x_train2, y_train2 = x_train[val_size:], train_labels[val_size:]\n", "\n", "print(\"\\nAfter preprocessing:\")\n", "print(\"  x_train2:\", x_train2.shape, \" x_val:\", x_val.shape, \" x_test:\", x_test.shape)\n", "print(\"Pixel range AFTER scaling:\", float(x_train2.min()), \"to\", float(x_train2.max()))"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# Pixel histogram AFTER scaling\n", "rand_pixels_scaled = train_images[:500].reshape(-1)\n", "plt.figure(figsize=(6,4))\n", "plt.hist(rand_pixels_scaled, bins=30)\n", "plt.xlabel(\"Pixel intensity (0..1)\")\n", "plt.ylabel(\"Frequency\")\n", "plt.title(\"Pixel histogram AFTER scaling\")\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 7) Concept: Flatten vs Image Grid & What is an MLP?\n", "**Image Grid:** Each image is `(28, 28)` \u2014 2D pixels.  \n", "**Flatten:** Convert to a **1D vector of 784** so a Dense layer can take all pixels as input.\n", "\n", "**MLP (Multi-Layer Perceptron):** A feedforward neural network:  \n", "- **Input layer:** 784 features (flattened pixels)  \n", "- **Hidden layers:** Dense layers with non-linear activations (e.g., ReLU)  \n", "- **Output layer:** 10 neurons with **Softmax** \u2192 class probabilities\n", "\n", "We\u2019ll build: `Input(784) \u2192 Dense(512, ReLU) \u2192 Dropout(0.3) \u2192 Dense(256, ReLU) \u2192 Dense(10, Softmax)`"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 8) Build, Compile, Train \u2014 Single MLP"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["model = keras.Sequential([\n", "    layers.Input(shape=(784,)),\n", "    layers.Dense(512, activation=\"relu\"),\n", "    layers.Dropout(0.30),\n", "    layers.Dense(256, activation=\"relu\"),\n", "    layers.Dense(10, activation=\"softmax\")\n", "], name=\"mlp_fashion\")\n", "\n", "model.summary()\n", "\n", "model.compile(\n", "    optimizer=keras.optimizers.Adam(1e-3),\n", "    loss=\"sparse_categorical_crossentropy\",\n", "    metrics=[\"accuracy\"]\n", ")\n", "\n", "EPOCHS = 12\n", "BATCH  = 128\n", "history = model.fit(\n", "    x_train2, y_train2,\n", "    validation_data=(x_val, y_val),\n", "    epochs=EPOCHS,\n", "    batch_size=BATCH,\n", "    verbose=2\n", ")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 9) Training Curves (Loss & Accuracy)"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["plt.figure(figsize=(6,4))\n", "plt.plot(history.history[\"loss\"], label=\"train_loss\")\n", "plt.plot(history.history[\"val_loss\"], label=\"val_loss\")\n", "plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\")\n", "plt.title(\"Training vs Validation Loss\")\n", "plt.legend(); plt.tight_layout(); plt.show()\n", "\n", "plt.figure(figsize=(6,4))\n", "plt.plot(history.history[\"accuracy\"], label=\"train_acc\")\n", "plt.plot(history.history[\"val_accuracy\"], label=\"val_acc\")\n", "plt.xlabel(\"Epoch\"); plt.ylabel(\"Accuracy\")\n", "plt.title(\"Training vs Validation Accuracy\")\n", "plt.legend(); plt.tight_layout(); plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 10) Evaluate on Test Set"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["test_loss, test_acc = model.evaluate(x_test, test_labels, verbose=0)\n", "print(f\"Test accuracy: {test_acc:.4f} | Test loss: {test_loss:.4f}\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 11) Confusion Matrix & Classification Report"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["y_prob = model.predict(x_test, verbose=0)\n", "y_pred = np.argmax(y_prob, axis=1)\n", "\n", "cm = confusion_matrix(test_labels, y_pred)\n", "plt.figure(figsize=(8,6))\n", "plt.imshow(cm)\n", "plt.title(\"Confusion Matrix (Test)\")\n", "plt.xlabel(\"Predicted\"); plt.ylabel(\"True\")\n", "plt.xticks(range(10), class_names, rotation=45, ha=\"right\")\n", "plt.yticks(range(10), class_names)\n", "for i in range(10):\n", "    for j in range(10):\n", "        plt.text(j, i, cm[i, j], ha=\"center\", va=\"center\", fontsize=9)\n", "plt.colorbar(); plt.tight_layout(); plt.show()\n", "\n", "print(\"\\nClassification report:\")\n", "print(classification_report(test_labels, y_pred, target_names=class_names, digits=4))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 12) Misclassified Examples (Lowest Confidence)"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["wrong_idx = np.where(y_pred != test_labels)[0]\n", "if len(wrong_idx) > 0:\n", "    probs_wrong = y_prob[wrong_idx]\n", "    max_probs = probs_wrong.max(axis=1)\n", "    order = np.argsort(max_probs)  # smallest confidence first\n", "    N_SHOW = min(12, len(order))\n", "    sel = wrong_idx[order[:N_SHOW]]\n", "\n", "    rows, cols = 3, 4\n", "    plt.figure(figsize=(cols*2.2, rows*2.2))\n", "    for k, idx in enumerate(sel):\n", "        plt.subplot(rows, cols, k+1)\n", "        plt.imshow(test_images[idx], cmap=\"gray\")\n", "        pred = y_pred[idx]; true = test_labels[idx]; conf = y_prob[idx, pred]\n", "        plt.title(f\"pred={class_names[pred]}\\ntrue={class_names[true]}\\np={conf:.2f}\", fontsize=9)\n", "        plt.axis(\"off\")\n", "    plt.suptitle(\"Hard Misclassifications (lowest confidence)\")\n", "    plt.tight_layout(); plt.show()\n", "else:\n", "    print(\"No misclassifications (unlikely).\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 13) Save & Reload (Basic Deployment Step)"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["model.save(\"fashion_mnist_single_mlp.keras\")\n", "reloaded = keras.models.load_model(\"fashion_mnist_single_mlp.keras\")\n", "print(\"Reloaded OK. Params:\", reloaded.count_params())"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.x"}, "colab": {"name": "FashionMNIST_SingleMLP_Colab.ipynb", "provenance": []}, "created": "2025-08-11T04:56:40.025532Z"}, "nbformat": 4, "nbformat_minor": 5}