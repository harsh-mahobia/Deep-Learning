{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07e40cf0",
   "metadata": {},
   "source": [
    "# ðŸ§  Vanishing Gradient in Deep Learning (Practical Example)\n",
    "\n",
    "This notebook uses the **California Housing Dataset** to simulate how the vanishing gradient problem can occur in deep networks. We'll compare a deep model with `sigmoid` activations (that suffers from vanishing gradient) versus one with `ReLU` activations (that handles it well)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701ca53a",
   "metadata": {},
   "source": [
    "## ðŸ” Step 1: Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f076dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load dataset\n",
    "data = fetch_california_housing()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Preprocess\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "y = y.reshape(-1, 1)  # ensure it's column vector\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7eea7c",
   "metadata": {},
   "source": [
    "## ðŸ—ï¸ Step 2: Build Deep Model with Sigmoid (to show vanishing gradient)\n",
    "We'll build a 10-layer network with `sigmoid` activations and monitor gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0544b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_sigmoid = tf.keras.Sequential()\n",
    "model_sigmoid.add(tf.keras.Input(shape=(X_train.shape[1],)))\n",
    "for _ in range(10):\n",
    "    model_sigmoid.add(tf.keras.layers.Dense(64, activation='sigmoid'))\n",
    "model_sigmoid.add(tf.keras.layers.Dense(1))\n",
    "model_sigmoid.compile(optimizer='sgd', loss='mse')\n",
    "model_sigmoid.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89be35e",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Step 3: Train Model and Log Gradients using GradientTape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33af265d",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "all_gradients = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    with tf.GradientTape() as tape:\n",
    "        preds = model_sigmoid(X_train[:512], training=True)\n",
    "        loss = tf.keras.losses.mean_squared_error(y_train[:512], preds)\n",
    "\n",
    "    grads = tape.gradient(loss, model_sigmoid.trainable_variables)\n",
    "    grad_norms = [tf.norm(g).numpy() if g is not None else 0 for g in grads if len(g.shape) > 1]\n",
    "    all_gradients.append(grad_norms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44869fe4",
   "metadata": {},
   "source": [
    "## ðŸ“‰ Step 4: Visualize Gradient Norms to Detect Vanishing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560c4255",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "for i, norms in enumerate(all_gradients):\n",
    "    plt.plot(norms, label=f\"Epoch {i+1}\", marker='o')\n",
    "\n",
    "plt.title(\"Gradient Norms across Layers (Sigmoid)\")\n",
    "plt.xlabel(\"Layer Index (from output to input)\")\n",
    "plt.ylabel(\"Gradient L2 Norm\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a262c66d",
   "metadata": {},
   "source": [
    "## âš¡ Step 5: Build and Compare with ReLU Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2832bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_relu = tf.keras.Sequential()\n",
    "model_relu.add(tf.keras.Input(shape=(X_train.shape[1],)))\n",
    "for _ in range(10):\n",
    "    model_relu.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "model_relu.add(tf.keras.layers.Dense(1))\n",
    "model_relu.compile(optimizer='sgd', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5c18d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_gradients_relu = []\n",
    "for epoch in range(epochs):\n",
    "    with tf.GradientTape() as tape:\n",
    "        preds = model_relu(X_train[:512], training=True)\n",
    "        loss = tf.keras.losses.mean_squared_error(y_train[:512], preds)\n",
    "\n",
    "    grads = tape.gradient(loss, model_relu.trainable_variables)\n",
    "    grad_norms = [tf.norm(g).numpy() if g is not None else 0 for g in grads if len(g.shape) > 1]\n",
    "    all_gradients_relu.append(grad_norms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0079fb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "for i, norms in enumerate(all_gradients_relu):\n",
    "    plt.plot(norms, label=f\"Epoch {i+1}\", marker='x')\n",
    "\n",
    "plt.title(\"Gradient Norms across Layers (ReLU)\")\n",
    "plt.xlabel(\"Layer Index (from output to input)\")\n",
    "plt.ylabel(\"Gradient L2 Norm\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2d7f24",
   "metadata": {},
   "source": [
    "## ðŸ§  Final Explanation\n",
    "- In the **sigmoid model**, you should see gradient magnitudes **decrease** as you move toward early layers.\n",
    "- In the **ReLU model**, the gradients stay **stronger and stable**, indicating healthier learning.\n",
    "\n",
    "This shows how activation choice affects learning in deep networks â€” and where vanishing gradient occurs."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
